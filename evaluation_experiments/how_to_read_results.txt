# ğŸ¯ EINFACHER GRID SEARCH INTERPRETATION GUIDE

"""
ğŸ“Š WIE LESE ICH DIE GRID SEARCH ERGEBNISSE?

Stelle dir vor du testest verschiedene Chatbot-Einstellungen.
Du willst wissen: "Welche Einstellung gibt die BESTEN Antworten?"

Es gibt 3 Arten von schlechten Chatbots:
1. ğŸ¤¥ LÃœGNER: Erfindet Informationen (hohe Hallucination)
2. ğŸ˜´ LANGWEILER: Gibt wenig nÃ¼tzliche Info (niedrige Informativeness) 
3. ğŸ™ˆ VERWEIGERER: Sagt zu oft "WeiÃŸ nicht" (hohe Evasiveness)

Du willst: EHRLICH + HILFREICH + MUTIG
"""

# ===== DIE 3 HAUPT-METRIKEN =====

"""
ğŸ¯ avg_hallucination_score (1-5) - "WIE EHRLICH?"
   1-2 = ğŸ˜‡ Sehr ehrlich (erfindet nichts)
   3   = ğŸ˜ Okay (manchmal ungenau)  
   4-5 = ğŸ¤¥ LÃ¼gner (erfindet viel)
   
ğŸ“ˆ avg_informativeness_score (1-5) - "WIE HILFREICH?"
   4-5 = ğŸŒŸ Sehr hilfreich (gibt gute Details)
   3   = ğŸ˜ Okay hilfreich 
   1-2 = ğŸ˜´ Wenig hilfreich (sagt wenig)
   
âš–ï¸ evasiveness_rate (0-100%) - "WIE MUTIG?"
   0-20%  = ğŸ’ª Mutig (antwortet meist)
   20-50% = ğŸ˜ Okay mutig
   50%+   = ğŸ™ˆ Zu Ã¤ngstlich (sagt oft "WeiÃŸ nicht")
"""

# ===== BEISPIEL INTERPRETATION =====

def interpret_experiment_results(experiment_row):
    """
    Vereinfachte Interpretation eines Grid Search Experiments
    """
    
    # Hole die Werte
    halluc_score = experiment_row.get('avg_hallucination_score', None)
    info_score = experiment_row.get('avg_informativeness_score', None) 
    evasive_rate = experiment_row.get('evasiveness_rate', None)
    
    # Bewerte jeden Aspekt
    honesty = interpret_hallucination(halluc_score)
    helpfulness = interpret_informativeness(info_score)
    courage = interpret_evasiveness(evasive_rate)
    
    # Gesamtbewertung
    overall = determine_overall_quality(honesty, helpfulness, courage)
    
    return {
        'honesty': honesty,
        'helpfulness': helpfulness, 
        'courage': courage,
        'overall': overall,
        'recommendation': get_recommendation(overall)
    }

def interpret_hallucination(score):
    if score is None:
        return "â“ Unbekannt"
    elif score <= 2.0:
        return "ğŸ˜‡ Sehr ehrlich"
    elif score <= 3.0:
        return "ğŸ˜ Okay ehrlich"
    elif score <= 4.0:
        return "ğŸ˜¬ Manchmal unehrlich"
    else:
        return "ğŸ¤¥ Oft unehrlich"

def interpret_informativeness(score):
    if score is None:
        return "â“ Unbekannt"
    elif score >= 4.0:
        return "ğŸŒŸ Sehr hilfreich"
    elif score >= 3.0:
        return "ğŸ˜ Okay hilfreich"
    elif score >= 2.0:
        return "ğŸ˜´ Wenig hilfreich"
    else:
        return "ğŸ’¤ Sehr wenig hilfreich"

def interpret_evasiveness(rate):
    if rate is None:
        return "â“ Unbekannt"
    elif rate <= 20:
        return "ğŸ’ª Mutig"
    elif rate <= 50:
        return "ğŸ˜ Okay mutig"
    else:
        return "ğŸ™ˆ Zu Ã¤ngstlich"

def determine_overall_quality(honesty, helpfulness, courage):
    # Einfache Regeln fÃ¼r Gesamtbewertung
    if "Sehr ehrlich" in honesty and "Sehr hilfreich" in helpfulness:
        return "ğŸ† EXCELLENT"
    elif "ehrlich" in honesty and "hilfreich" in helpfulness and "Mutig" in courage:
        return "âœ… GOOD"
    elif "unehrlich" in honesty:
        return "âŒ PROBLEMATIC - Erfindet zu viel"
    elif "Ã¤ngstlich" in courage:
        return "âš ï¸ TOO CONSERVATIVE - Antwortet zu wenig"
    else:
        return "ğŸ˜ AVERAGE"

def get_recommendation(overall):
    recommendations = {
        "ğŸ† EXCELLENT": "ğŸ‘ Diese Einstellung verwenden!",
        "âœ… GOOD": "ğŸ‘Œ Gute Einstellung, kann verwendet werden",
        "ğŸ˜ AVERAGE": "ğŸ¤” Okay, aber vielleicht gibt es bessere",
        "âš ï¸ TOO CONSERVATIVE": "ğŸ”§ Weniger konservative Parameter probieren",
        "âŒ PROBLEMATIC": "ğŸš« Nicht verwenden - zu unzuverlÃ¤ssig"
    }
    return recommendations.get(overall, "ğŸ¤· Unbekannt")

# ===== PRAKTISCHES BEISPIEL =====

"""
BEISPIEL GRID SEARCH ERGEBNIS:

EXP_001 | WITH_CONTEXT | conservative | 1.8 | 2.1 | 65% | âš ï¸ TOO CONSERVATIVE
EXP_002 | WITH_CONTEXT | standard     | 2.3 | 3.8 | 25% | âœ… GOOD  
EXP_003 | WITH_CONTEXT | creative     | 3.9 | 4.2 | 10% | âŒ PROBLEMATIC
EXP_004 | WITHOUT_CONTEXT | standard  | 2.1 | 2.9 | 35% | ğŸ˜ AVERAGE

INTERPRETATION:
- EXP_001: Sehr ehrlich aber zu Ã¤ngstlich (sagt oft "WeiÃŸ nicht")
- EXP_002: ğŸ‘ BESTE WAHL! Gute Balance aus allen 3 Aspekten
- EXP_003: Sehr hilfreich aber erfindet zu viel
- EXP_004: Durchschnittlich in allem

EMPFEHLUNG: Verwende EXP_002 (WITH_CONTEXT + standard parameters)
"""

# ===== VEREINFACHTE ANALYSE FUNKTION =====

def simple_grid_analysis(grid_search_results_file):
    """
    Liest Grid Search Ergebnisse und gibt einfache Interpretation
    """
    import pandas as pd
    
    # Lade Ergebnisse
    df = pd.read_excel(grid_search_results_file, sheet_name='All_Experiments')
    
    print("ğŸ¯ EINFACHE GRID SEARCH ANALYSE")
    print("=" * 50)
    
    # Nur completed experiments
    completed = df[df['status'] == 'completed']
    
    if len(completed) == 0:
        print("âŒ Keine erfolgreichen Experimente gefunden!")
        return
    
    print(f"ğŸ“Š Analysiere {len(completed)} Experimente\n")
    
    # Interpretiere jedes Experiment
    for idx, exp in completed.iterrows():
        interpretation = interpret_experiment_results(exp)
        
        print(f"ğŸ§ª {exp['experiment_id']}: {exp['prompt_name']} + {exp['param_set_name']}")
        print(f"   ğŸ˜‡ Ehrlichkeit: {interpretation['honesty']}")
        print(f"   ğŸŒŸ Hilfsbereitschaft: {interpretation['helpfulness']}")  
        print(f"   ğŸ’ª Mut: {interpretation['courage']}")
        print(f"   ğŸ† Gesamt: {interpretation['overall']}")
        print(f"   ğŸ’¡ Empfehlung: {interpretation['recommendation']}")
        print()
    
    # Beste Empfehlung
    best_experiments = completed[
        (completed['avg_hallucination_score'] <= 2.5) & 
        (completed['avg_informativeness_score'] >= 3.0) &
        (completed['evasiveness_rate'] <= 40)
    ]
    
    if len(best_experiments) > 0:
        best = best_experiments.iloc[0]
        print("ğŸ† BESTE EMPFEHLUNG:")
        print(f"   {best['experiment_id']}: {best['prompt_name']} + {best['param_set_name']}")
        print(f"   ğŸ‘ Diese Einstellung in Produktion verwenden!")
    else:
        print("âš ï¸ Kein ideales Experiment gefunden. Probiere andere Parameter!")

# ===== WIE VERWENDEN? =====

"""
# Nach einem Grid Search run:
simple_grid_analysis('evaluation_experiments/grid_search_results/grid_search_summary.xlsx')

# Das gibt dir eine einfache, verstÃ¤ndliche Analyse wie:
ğŸ§ª EXP_002: WITH HIERARCHICAL CONTEXT + standard
   ğŸ˜‡ Ehrlichkeit: Sehr ehrlich  
   ğŸŒŸ Hilfsbereitschaft: Okay hilfreich
   ğŸ’ª Mut: Mutig
   ğŸ† Gesamt: GOOD
   ğŸ’¡ Empfehlung: Gute Einstellung, kann verwendet werden

ğŸ† BESTE EMPFEHLUNG:
   EXP_002: WITH HIERARCHICAL CONTEXT + standard
   ğŸ‘ Diese Einstellung in Produktion verwenden!
"""