# 🎯 EINFACHER GRID SEARCH INTERPRETATION GUIDE

"""
📊 WIE LESE ICH DIE GRID SEARCH ERGEBNISSE?

Stelle dir vor du testest verschiedene Chatbot-Einstellungen.
Du willst wissen: "Welche Einstellung gibt die BESTEN Antworten?"

Es gibt 3 Arten von schlechten Chatbots:
1. 🤥 LÜGNER: Erfindet Informationen (hohe Hallucination)
2. 😴 LANGWEILER: Gibt wenig nützliche Info (niedrige Informativeness) 
3. 🙈 VERWEIGERER: Sagt zu oft "Weiß nicht" (hohe Evasiveness)

Du willst: EHRLICH + HILFREICH + MUTIG
"""

# ===== DIE 3 HAUPT-METRIKEN =====

"""
🎯 avg_hallucination_score (1-5) - "WIE EHRLICH?"
   1-2 = 😇 Sehr ehrlich (erfindet nichts)
   3   = 😐 Okay (manchmal ungenau)  
   4-5 = 🤥 Lügner (erfindet viel)
   
📈 avg_informativeness_score (1-5) - "WIE HILFREICH?"
   4-5 = 🌟 Sehr hilfreich (gibt gute Details)
   3   = 😐 Okay hilfreich 
   1-2 = 😴 Wenig hilfreich (sagt wenig)
   
⚖️ evasiveness_rate (0-100%) - "WIE MUTIG?"
   0-20%  = 💪 Mutig (antwortet meist)
   20-50% = 😐 Okay mutig
   50%+   = 🙈 Zu ängstlich (sagt oft "Weiß nicht")
"""

# ===== BEISPIEL INTERPRETATION =====

def interpret_experiment_results(experiment_row):
    """
    Vereinfachte Interpretation eines Grid Search Experiments
    """
    
    # Hole die Werte
    halluc_score = experiment_row.get('avg_hallucination_score', None)
    info_score = experiment_row.get('avg_informativeness_score', None) 
    evasive_rate = experiment_row.get('evasiveness_rate', None)
    
    # Bewerte jeden Aspekt
    honesty = interpret_hallucination(halluc_score)
    helpfulness = interpret_informativeness(info_score)
    courage = interpret_evasiveness(evasive_rate)
    
    # Gesamtbewertung
    overall = determine_overall_quality(honesty, helpfulness, courage)
    
    return {
        'honesty': honesty,
        'helpfulness': helpfulness, 
        'courage': courage,
        'overall': overall,
        'recommendation': get_recommendation(overall)
    }

def interpret_hallucination(score):
    if score is None:
        return "❓ Unbekannt"
    elif score <= 2.0:
        return "😇 Sehr ehrlich"
    elif score <= 3.0:
        return "😐 Okay ehrlich"
    elif score <= 4.0:
        return "😬 Manchmal unehrlich"
    else:
        return "🤥 Oft unehrlich"

def interpret_informativeness(score):
    if score is None:
        return "❓ Unbekannt"
    elif score >= 4.0:
        return "🌟 Sehr hilfreich"
    elif score >= 3.0:
        return "😐 Okay hilfreich"
    elif score >= 2.0:
        return "😴 Wenig hilfreich"
    else:
        return "💤 Sehr wenig hilfreich"

def interpret_evasiveness(rate):
    if rate is None:
        return "❓ Unbekannt"
    elif rate <= 20:
        return "💪 Mutig"
    elif rate <= 50:
        return "😐 Okay mutig"
    else:
        return "🙈 Zu ängstlich"

def determine_overall_quality(honesty, helpfulness, courage):
    # Einfache Regeln für Gesamtbewertung
    if "Sehr ehrlich" in honesty and "Sehr hilfreich" in helpfulness:
        return "🏆 EXCELLENT"
    elif "ehrlich" in honesty and "hilfreich" in helpfulness and "Mutig" in courage:
        return "✅ GOOD"
    elif "unehrlich" in honesty:
        return "❌ PROBLEMATIC - Erfindet zu viel"
    elif "ängstlich" in courage:
        return "⚠️ TOO CONSERVATIVE - Antwortet zu wenig"
    else:
        return "😐 AVERAGE"

def get_recommendation(overall):
    recommendations = {
        "🏆 EXCELLENT": "👍 Diese Einstellung verwenden!",
        "✅ GOOD": "👌 Gute Einstellung, kann verwendet werden",
        "😐 AVERAGE": "🤔 Okay, aber vielleicht gibt es bessere",
        "⚠️ TOO CONSERVATIVE": "🔧 Weniger konservative Parameter probieren",
        "❌ PROBLEMATIC": "🚫 Nicht verwenden - zu unzuverlässig"
    }
    return recommendations.get(overall, "🤷 Unbekannt")

# ===== PRAKTISCHES BEISPIEL =====

"""
BEISPIEL GRID SEARCH ERGEBNIS:

EXP_001 | WITH_CONTEXT | conservative | 1.8 | 2.1 | 65% | ⚠️ TOO CONSERVATIVE
EXP_002 | WITH_CONTEXT | standard     | 2.3 | 3.8 | 25% | ✅ GOOD  
EXP_003 | WITH_CONTEXT | creative     | 3.9 | 4.2 | 10% | ❌ PROBLEMATIC
EXP_004 | WITHOUT_CONTEXT | standard  | 2.1 | 2.9 | 35% | 😐 AVERAGE

INTERPRETATION:
- EXP_001: Sehr ehrlich aber zu ängstlich (sagt oft "Weiß nicht")
- EXP_002: 👍 BESTE WAHL! Gute Balance aus allen 3 Aspekten
- EXP_003: Sehr hilfreich aber erfindet zu viel
- EXP_004: Durchschnittlich in allem

EMPFEHLUNG: Verwende EXP_002 (WITH_CONTEXT + standard parameters)
"""

# ===== VEREINFACHTE ANALYSE FUNKTION =====

def simple_grid_analysis(grid_search_results_file):
    """
    Liest Grid Search Ergebnisse und gibt einfache Interpretation
    """
    import pandas as pd
    
    # Lade Ergebnisse
    df = pd.read_excel(grid_search_results_file, sheet_name='All_Experiments')
    
    print("🎯 EINFACHE GRID SEARCH ANALYSE")
    print("=" * 50)
    
    # Nur completed experiments
    completed = df[df['status'] == 'completed']
    
    if len(completed) == 0:
        print("❌ Keine erfolgreichen Experimente gefunden!")
        return
    
    print(f"📊 Analysiere {len(completed)} Experimente\n")
    
    # Interpretiere jedes Experiment
    for idx, exp in completed.iterrows():
        interpretation = interpret_experiment_results(exp)
        
        print(f"🧪 {exp['experiment_id']}: {exp['prompt_name']} + {exp['param_set_name']}")
        print(f"   😇 Ehrlichkeit: {interpretation['honesty']}")
        print(f"   🌟 Hilfsbereitschaft: {interpretation['helpfulness']}")  
        print(f"   💪 Mut: {interpretation['courage']}")
        print(f"   🏆 Gesamt: {interpretation['overall']}")
        print(f"   💡 Empfehlung: {interpretation['recommendation']}")
        print()
    
    # Beste Empfehlung
    best_experiments = completed[
        (completed['avg_hallucination_score'] <= 2.5) & 
        (completed['avg_informativeness_score'] >= 3.0) &
        (completed['evasiveness_rate'] <= 40)
    ]
    
    if len(best_experiments) > 0:
        best = best_experiments.iloc[0]
        print("🏆 BESTE EMPFEHLUNG:")
        print(f"   {best['experiment_id']}: {best['prompt_name']} + {best['param_set_name']}")
        print(f"   👍 Diese Einstellung in Produktion verwenden!")
    else:
        print("⚠️ Kein ideales Experiment gefunden. Probiere andere Parameter!")

# ===== WIE VERWENDEN? =====

"""
# Nach einem Grid Search run:
simple_grid_analysis('evaluation_experiments/grid_search_results/grid_search_summary.xlsx')

# Das gibt dir eine einfache, verständliche Analyse wie:
🧪 EXP_002: WITH HIERARCHICAL CONTEXT + standard
   😇 Ehrlichkeit: Sehr ehrlich  
   🌟 Hilfsbereitschaft: Okay hilfreich
   💪 Mut: Mutig
   🏆 Gesamt: GOOD
   💡 Empfehlung: Gute Einstellung, kann verwendet werden

🏆 BESTE EMPFEHLUNG:
   EXP_002: WITH HIERARCHICAL CONTEXT + standard
   👍 Diese Einstellung in Produktion verwenden!
"""