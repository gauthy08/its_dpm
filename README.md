## ITS datapoint description/data catalogue 

# Beschreibung
Mit diesem Projekt wird mit Hilfe des OeNB Chatbots Beschreibungen f√ºr ITS Datenpunkte generiert, welche im Anschluss an ISIS und infolgedessen an den Datenkatalog hochgeladen werden. 

# Voraussetzung
Mit den folgenden Einstellungen sollte das Projekt ausf√ºhrbar sein
Python 3.12
Enable Spark
Spark 3.3.2 - CDP 7.1.9.0 

# Benutzung
Start:
im Terminal "python main.py" aufrufen. Dann wird ein "Men√º" angezeicht mit verf√ºgbaren Funktionen. 

## 1: Tabellen erstellen/create_tables()
Das Projekt beinhaltet eine eigene kleine Datenbank databank.db mit sqlalchemy. Falls man diese komplett l√∂scht, k√∂nnen die Tabellen neu erstellt werden. Ist aber im Standardfall nicht notwendig. Kann man also grunds√§tzlich ignorieren, sondern muss/kann nur ausgef√ºhrt werden, wenn man bei "Null" beginnt. 
Noch offenes TO DO: entferne Tabellen nicht mehr gebraucht werden. 

## 2: ITS Base Data laden/load_hue_its()
Diese Funktion l√§dt ITS-Basisdaten aus der Hive-Datenbank its_analysedaten_prod √ºber eine SparkSession mit HiveWarehouseConnector. Die Daten werden direkt bei der Abfrage auf die zwei relevanten Taxonomy-Codes FINREP_3.2.1 und COREP_3.2 gefiltert, um die Datenmenge zu reduzieren (ca. 81.000 Zeilen). Nach der Abfrage werden die Spark-DataFrames in Pandas DataFrames konvertiert und anschlie√üend in die lokale SQL-Datenbank-Tabelle ITSBaseData_new geschrieben. Die Funktion bietet dabei interaktive Optionen zum Umgang mit bereits vorhandenen Daten (l√∂schen, hinzuf√ºgen oder abbrechen) und zeigt w√§hrend des gesamten Prozesses Fortschrittsmeldungen an. Ein NumPy-Kompatibilit√§ts-Workaround sorgt f√ºr die reibungslose Konvertierung zwischen √§lteren PySpark- und neueren NumPy-Versionen.
Muss nicht immer ausgef√ºhrt werden, da die Daten ja in der Projekt-Datenbank erhalten bleiben. Muss nur durchgef√ºhrt werden, wenn beispielsweise ein neues Framework (Finrep oder Corep) verwendet wird. 

## 3: DPM_TableStructure hochladen/Hierachie
`load_tablestructurehierarchy(file_path)` konvertiert flache Excel-Tabellenstrukturen (qDPM_TableStructure) aus dem Ordner "data" in hierarchische Baumstrukturen f√ºr z.B. COREP 3.2 Regulatory Reporting. Erstellt f√ºr jeden TableCode zwei separate B√§ume - einen f√ºr Zeilen ("Table row") und einen f√ºr Spalten ("Table column") - basierend auf den Parent-Child-Beziehungen √ºber OrdinateID/ParentOrdinateID. Serialisiert alle generierten Baumstrukturen als Dictionary in eine Pickle-Datei (baumstruktur_XXX.pkl) im Ordner tree_structures f√ºr sp√§tere Verwendung. Diese Baumstrukturen werden genutzt, um hierarchische Kontexte f√ºr die RAG-basierte Textgenerierung zu erstellen - jeder Datenpunkt kann so mit seinem vollst√§ndigen hierarchischen Pfad beschrieben werden.
Siehe "README_TREE_VIEWER.md": Damit kann man die Baumstruktur visualisieren/analysieren zum besseren Verst√§ndnis. 

## 4: PRODUCTION: ChatBot-Beschreibungen generieren 

### Zweck
Generiert automatisch KI-basierte Erkl√§rungen f√ºr regulatorische Datenpunkte aus COREP/FINREP-Templates mithilfe eines LLM (Mistral).
Was passiert:
	1. L√§dt Baumstruktur aus Pickle-File (z.B. alle COREP-Tabellen mit hierarchischen Beziehungen)
	2. Durchl√§uft jeden Datenpunkt im Baum (z.B. "0010: Common Equity Tier 1 capital")
	3. Erstellt Kontext mit dem hierarchischen Pfad (Parent ‚Üí Child Beziehungen)
	4. Sendet an KI-API mit RAG-Knowledge-Base f√ºr regulatorisches Wissen
	5. Erh√§lt Beschreibung zur√ºck (max. 100 W√∂rter, faktisch, ohne Halluzinationen)
	6. Speichert Ergebnisse als Excel in production_runs/
### Input
	‚Ä¢ Pickle-File mit Baumstrukturen
	‚Ä¢ Prompt-Template (definiert Antwortformat)
	‚Ä¢ Scope (welche Tabellen verarbeiten)
### Output
Excel-File mit:
	‚Ä¢ Alle Datenpunkte + ihre generierten Beschreibungen
	‚Ä¢ Hierarchischer Kontext pro Datenpunkt
	‚Ä¢ Metadaten (Verarbeitungszeit, verwendete Parameter)
Beispiel-Ergebnis:
Node: "0010 - Common Equity Tier 1 capital"
Generated: "Common Equity Tier 1 capital refers to the highest quality 
           regulatory capital that banks must hold, consisting primarily 
           of common shares and retained earnings..."

offene TODO: Men√ºpunkt-Auswahl steckt aktuell in in main.py. K√∂nnte man zur besseren √úbersicht in eine eigene Funktion stecken. 
Output-pfad/files beschreiben anpassen. 


## 5: Corep Annex 5 extrahieren


## 4: üöÄ PRODUCTION: ChatBot-Beschreibungen generieren


???Men√ºpunkt 1: Die Funktion `create_tables()` erstellt automatisch alle in den SQLAlchemy-Modellen definierten Tabellen, **sofern sie noch nicht existieren**. Bereits bestehende Tabellen bleiben **unver√§ndert**. 

???Men√ºpunkt 2: `load_hue_its()` ist eine ETL-Funktion, die Regulatory Reporting Daten aus der HUE-Datenbank its_analysedaten_wapr.its_base_data √ºber Spark/Hive Warehouse Connector l√§dt. Filtert automatisch nach FINREP 3.2.1 und COREP 3.2 Taxonomien und bietet interaktive Optionen f√ºr den Umgang mit bereits vorhandenen Daten (l√∂schen/erweitern/abbrechen). Schreibt die gefilterten Daten als Batch-Insert in die lokale SQL-Datenbank-Tabelle ITSBaseData_new.

???Men√ºpunkt 3: `load_tablestructurehierarchy(file_path)` konvertiert flache Excel-Tabellenstrukturen (qDPM_TableStructure) in hierarchische Baumstrukturen f√ºr z.B. COREP 3.2 Regulatory Reporting. Erstellt f√ºr jeden TableCode zwei separate B√§ume - einen f√ºr Zeilen ("Table row") und einen f√ºr Spalten ("Table column") - basierend auf den Parent-Child-Beziehungen √ºber OrdinateID/ParentOrdinateID. Serialisiert alle generierten Baumstrukturen als Dictionary in eine Pickle-Datei (baumstruktur_XXX.pkl) im Ordner tree_structures f√ºr sp√§tere Verwendung. TO DO: Corep oder Finrep soll nicht in der load-funktion sondern im Men√º ausgew√§hlt werden. 


# gitignore 


# Refactoring: 
to do: create_tables() nicht ben√∂tigte Tables entfernen bzw. l√∂schen